# .cursorrules.md
## üß© Project Overview
This project is a **Retrieval-Augmented Generation (RAG) system** for **compliance document analysis**.  
It integrates:
- OpenAI / LLM models for text generation,
- ChromaDB or similar for vector storage,
- FastAPI backend for document upload, retrieval, and compliance evaluation.

Cursor should follow the rules below when writing, refactoring, or generating new code.

---

## üß± 1. Code Quality & Maintainability Rules

### Formatting & Structure
- Use **Black** (Python formatter) and **isort** for imports.
- Maintain consistent indentation (4 spaces).
- Group related functions logically (e.g., `document_processing.py`, `llm_service.py`, `models.py`, `crud.py`).
- Keep functions under ~40 lines where possible.

### Naming Conventions
- Use descriptive, lowercase_with_underscores for functions and variables.
- Use PascalCase for classes.
- Avoid abbreviations unless standard (e.g., `db`, `api`, `llm`).

### Documentation
- Every function and class must include a **docstring** explaining purpose, inputs, and outputs.
- Use inline comments to explain complex logic (especially in retrieval and rule evaluation).

### Error Handling & Logging
- Wrap external API calls (LLM, Chroma) with `try/except` and meaningful error messages.
- Use Python‚Äôs built-in `logging` module for info, warnings, and errors.
- Do **not** use `print()` for debugging.

---

## üîê 2. Data Handling & Security Rules

### Input Validation
- Validate all uploaded documents for file type, size, and content type before processing.
- Reject unsupported or empty files gracefully.

### Secure Storage
- Store documents in a secure folder or cloud bucket.
- Use hashed filenames or UUIDs to prevent collisions.

### Environment Variables
- Store sensitive keys (e.g., `OPENAI_API_KEY`, `DB_URL`) in `.env` file.
- Never commit secrets to Git.

### Dependency Management
- Maintain dependencies in `requirements.txt` or `pyproject.toml`.
- Use compatible versions to avoid breaking changes.

### Data Privacy
- **Mask or redact sensitive data** (e.g., emails, IDs) before sending to LLM API.
- Ensure temporary data is deleted after use.

---

## üß† 3. RAG System Rules

### Chunking Strategy
- Use `RecursiveCharacterTextSplitter` for text chunking.
- Experiment with `chunk_size` and `chunk_overlap` for optimal retrieval performance.

### Embedding Management
- Use `OpenAIEmbeddings` or compatible model.
- Cache embeddings locally to avoid redundant calls.

### Vector Database
- Use ChromaDB for vector search.
- Include functions for:
  - Index creation and updates
  - Re-indexing on document change
  - Backup and restore of vector store

### Caching & Context
- Cache frequent queries (e.g., Redis or local cache).
- Log retrieved chunks with document IDs and similarity scores.
- Store context window (retrieved text) with LLM output for traceability.

### Prompt Engineering
- Store prompt templates in a dedicated file (`prompt_templates.py` or YAML config).
- Keep prompts clear, modular, and easy to update.

---

## ‚öñÔ∏è 4. Compliance & Rule Engine Rules

### Rule Definition
- Maintain all rules in a `/rules` directory.
- Each rule file defines:
  - `id`, `name`, `trigger_condition`, `action`, and `description`.

### Rule Registry
- Implement a `rule_registry` that auto-discovers and loads all rule modules at startup.

### Execution
- Rules can modify the LLM‚Äôs response, flag content, or add compliance annotations.

### Logging
- Log each applied rule (timestamp, rule ID, affected document).
- Keep a separate audit log file (`compliance_audit.log`).

### Testing
- Write unit tests for each rule to validate logic and expected outputs.

---

## üåê 5. API Design Rules

### RESTful Design
- Use **FastAPI** with clear endpoints:
  - `POST /upload` ‚Üí upload document
  - `GET /analyze` ‚Üí get compliance analysis
  - `POST /query` ‚Üí RAG query interface
- Follow naming: nouns for resources, verbs for actions.

### Responses
- Use Pydantic models for response schemas.
- Always return JSON with `success`, `message`, and `data`.

### Error Handling
- Return proper HTTP codes:
  - 400 for bad input,
  - 404 for not found,
  - 500 for server error.

### Background Tasks
- Offload long operations (e.g., chunking, embedding) using FastAPI BackgroundTasks or Celery if scaling.

---

## üìä 6. Evaluation & Monitoring Rules

### Performance Metrics
- Log latency of:
  - Document processing,
  - Retrieval,
  - LLM generation.

### Quality Metrics
- Periodically test retrieval accuracy using a fixed question set.
- Evaluate LLM outputs on relevance, correctness, and compliance accuracy.

### Monitoring
- Keep error rates and request counts in logs.
- Use tools like Prometheus or OpenTelemetry (optional for advanced setups).

---

## üß© 7. Collaboration & Project Hygiene

### Version Control
- Always create feature branches (`feature/rule-engine`, `fix/embedding-cache`).
- Use descriptive commit messages.

### Code Reviews
- All major changes must pass review before merge.

### Testing
- Run unit tests (`pytest`) before pushing.
- Aim for minimum 80% coverage.

---

**End of .cursorrules.md**
